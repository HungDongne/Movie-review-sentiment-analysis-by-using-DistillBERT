{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"eb9d05eefd254e418995899ac4c9c1bf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d017534c36154a439ea8c03f0c2ca9da","IPY_MODEL_de3d37c9e5d842dda63d17236d9e033a","IPY_MODEL_857e1e3c5da3423e9f8019c2b09525cc"],"layout":"IPY_MODEL_557cdf6f092e4d2dae31fcda06029a7b"}},"d017534c36154a439ea8c03f0c2ca9da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_31d9543b756b41c6bdb97661385701cf","placeholder":"​","style":"IPY_MODEL_13edb03a7cf74563bbc919904b5c643b","value":"config.json: 100%"}},"de3d37c9e5d842dda63d17236d9e033a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7d6c3e0ff2644079fca4053aedf2fcd","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fe9f83be3f10491b838173f27e1044c1","value":570}},"857e1e3c5da3423e9f8019c2b09525cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bfa70a5946a47fa837611cc364aa6a6","placeholder":"​","style":"IPY_MODEL_80bee8a80b0f4d488a98a3fe44e6ebb1","value":" 570/570 [00:00&lt;00:00, 37.4kB/s]"}},"557cdf6f092e4d2dae31fcda06029a7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31d9543b756b41c6bdb97661385701cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13edb03a7cf74563bbc919904b5c643b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7d6c3e0ff2644079fca4053aedf2fcd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe9f83be3f10491b838173f27e1044c1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9bfa70a5946a47fa837611cc364aa6a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80bee8a80b0f4d488a98a3fe44e6ebb1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ab6e524ddfb64e29a4bd2ee920289771":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_53c2ec1a56854d55af85cf00a1d4d05c","IPY_MODEL_1bfcfdfe4e854e1bab4cfc27bbb637d0","IPY_MODEL_13a0096c3c7340abbb7e0ef17ea8df6c"],"layout":"IPY_MODEL_103832108f354caab9bb473fd9581e02"}},"53c2ec1a56854d55af85cf00a1d4d05c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_621e3c9c1c2f43ecb5991046845fd5e6","placeholder":"​","style":"IPY_MODEL_6b6dbbf6b9f94e87a07b1bf74c05de6e","value":"model.safetensors: 100%"}},"1bfcfdfe4e854e1bab4cfc27bbb637d0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6a184936147494db4949b5797014e79","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0bfd162181c546b3b9a280b02113077c","value":440449768}},"13a0096c3c7340abbb7e0ef17ea8df6c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53bfcae6a7ec4bd6874f9ef9b4d96460","placeholder":"​","style":"IPY_MODEL_ad3586c75f38406fab68525bbde73b7d","value":" 440M/440M [00:02&lt;00:00, 178MB/s]"}},"103832108f354caab9bb473fd9581e02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"621e3c9c1c2f43ecb5991046845fd5e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b6dbbf6b9f94e87a07b1bf74c05de6e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d6a184936147494db4949b5797014e79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bfd162181c546b3b9a280b02113077c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"53bfcae6a7ec4bd6874f9ef9b4d96460":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad3586c75f38406fab68525bbde73b7d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets==1.18.4\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoTokenizer, AutoModel, DistilBertForSequenceClassification, DistilBertConfig, Trainer, TrainingArguments\nfrom transformers import get_cosine_schedule_with_warmup\nfrom datasets import load_metric\n\nfrom torch.optim import AdamW\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"_uuid":"b545a431-15de-440a-8ed2-0043de862df4","_cell_guid":"ea8002f8-12ad-41d8-9c6f-71a9f4097746","trusted":true,"id":"4utwMZhDn8Pe","outputId":"4e5d0a90-5bc3-4b7c-e91d-a507d9022086","execution":{"iopub.status.busy":"2024-12-03T16:12:15.359543Z","iopub.execute_input":"2024-12-03T16:12:15.359906Z","iopub.status.idle":"2024-12-03T16:12:33.078872Z","shell.execute_reply.started":"2024-12-03T16:12:15.359867Z","shell.execute_reply":"2024-12-03T16:12:33.077884Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets==1.18.4 in /opt/conda/lib/python3.10/site-packages (1.18.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==1.18.4) (1.26.4)\nRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==1.18.4) (16.1.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets==1.18.4) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==1.18.4) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==1.18.4) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==1.18.4) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==1.18.4) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==1.18.4) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->datasets==1.18.4) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==1.18.4) (3.9.5)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets==1.18.4) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==1.18.4) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets==1.18.4) (0.18.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==1.18.4) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==1.18.4) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==1.18.4) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==1.18.4) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==1.18.4) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==1.18.4) (4.0.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.18.4) (3.15.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.18.4) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.18.4) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==1.18.4) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==1.18.4) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==1.18.4) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==1.18.4) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==1.18.4) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==1.18.4) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==1.18.4) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==1.18.4) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==1.18.4) (1.16.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndf['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\nprint(df.head(5))","metadata":{"_uuid":"4d512451-3d7d-43b5-8764-4db478743902","_cell_guid":"ff75454d-24eb-4f04-871e-9f76d366dcbe","trusted":true,"collapsed":false,"id":"IBSPFHxawH2k","jupyter":{"outputs_hidden":false},"outputId":"166f3fd0-0ecb-4203-a270-527cf93bff3f","execution":{"iopub.status.busy":"2024-12-03T16:12:33.086397Z","iopub.execute_input":"2024-12-03T16:12:33.086684Z","iopub.status.idle":"2024-12-03T16:12:34.713665Z","shell.execute_reply.started":"2024-12-03T16:12:33.086658Z","shell.execute_reply":"2024-12-03T16:12:34.712702Z"}},"outputs":[{"name":"stdout","text":"                                              review  sentiment\n0  One of the other reviewers has mentioned that ...          1\n1  A wonderful little production. <br /><br />The...          1\n2  I thought this was a wonderful way to spend ti...          1\n3  Basically there's a family where a little boy ...          0\n4  Petter Mattei's \"Love in the Time of Money\" is...          1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def missing_values_analysis(df):\n    na_columns_ = [col for col in df.columns if df[col].isnull().sum() > 0]\n    n_miss = df[na_columns_].isnull().sum().sort_values(ascending=True)\n    ratio_ = (df[na_columns_].isnull().sum() / df.shape[0] * 100).sort_values(ascending=True)\n    missing_df = pd.concat([n_miss, np.round(ratio_, 2)], axis=1, keys=['Total Missing Values', 'Ratio'])\n    missing_df = pd.DataFrame(missing_df)\n    return missing_df","metadata":{"_uuid":"e10b0704-4cde-4bc0-840c-cd31a105b895","_cell_guid":"39349153-51df-4619-a4f1-e91a8516ceab","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-03T16:12:34.714934Z","iopub.execute_input":"2024-12-03T16:12:34.715331Z","iopub.status.idle":"2024-12-03T16:12:34.721358Z","shell.execute_reply.started":"2024-12-03T16:12:34.715290Z","shell.execute_reply":"2024-12-03T16:12:34.720495Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import spacy\nfrom nltk.corpus import stopwords\nimport re\nfrom bs4 import BeautifulSoup\n\n# Load spaCy's small English model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Initialize stopwords\nstop_words = set(stopwords.words(\"english\"))\n\ndef process(review):\n    # Remove HTML tags\n    review = BeautifulSoup(review, \"html.parser\").get_text()\n    \n    # Remove non-alphabetical characters (e.g., numbers, punctuation)\n    review = re.sub(r\"[^a-zA-Z]\", ' ', review)\n    \n    # Lowercase the text\n    review = review.lower()\n\n    review = re.sub(r'\\s+', ' ', review).strip()\n    \n    # Tokenization and Lemmatization with spaCy\n    doc = nlp(review)\n    review = [token.lemma_ for token in doc if token.text not in stop_words]\n    \n    return \" \".join(review)\n\ntrain_data = []\n\nfor i in range(len(df[\"review\"])):\n    if (i+1) % 2500 == 0:\n        print(\"Processed reviews:\", i+1)\n    \n    # train_data.append((df[\"review\"][i]))\n    train_data.append(process(df[\"review\"][i]))","metadata":{"_uuid":"7151afed-442e-4a58-ab82-f286b371b8b8","_cell_guid":"47909ed3-2501-4180-bafa-9120f6078346","trusted":true,"collapsed":false,"id":"K90ksD-JzO1B","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-03T16:12:34.722446Z","iopub.execute_input":"2024-12-03T16:12:34.722755Z"}},"outputs":[{"name":"stdout","text":"Processed reviews: 2500\nProcessed reviews: 5000\nProcessed reviews: 7500\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"train_texts, remaining_texts, train_labels, remaining_labels = train_test_split(\n    df['review'].tolist(),\n    df['sentiment'].tolist(),\n    test_size=0.2,\n    random_state=42\n)\n\nval_texts, test_texts, val_labels, test_labels = train_test_split(\n    remaining_texts,\n    remaining_labels,\n    test_size=0.2,\n    random_state=42\n)\n\nprint(f\"Training set size: {len(train_texts)}\")\nprint(f\"Validation set size: {len(val_texts)}\")\nprint(f\"Test set size: {len(test_texts)}\")","metadata":{"_uuid":"94a1e5c2-e912-41c7-af28-beae49093bf8","_cell_guid":"be4e0c1b-0910-4ddb-b784-c2f796350b75","trusted":true,"collapsed":false,"id":"1Jdyvl7RP0bD","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer, Trainer, TrainingArguments\nfrom datasets import load_dataset\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nsimcse_model = AutoModel.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")\nsimcse_tokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n\nclass ContrastiveLoss(nn.Module):\n    def __init__(self, temperature=0.05):\n        super(ContrastiveLoss, self).__init__()\n        self.temperature = temperature\n\n    def forward(self, anchor, positive):\n        sim = F.cosine_similarity(anchor, positive)\n        sim = sim / self.temperature\n        loss = -torch.log(torch.exp(sim) / torch.sum(torch.exp(sim), dim=-1))\n        return loss.mean()\n\nclass SimCSETrainer(Trainer):\n    def __init__(self, *args, contrastive_loss_fn, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.contrastive_loss_fn = contrastive_loss_fn\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        input_ids = inputs[\"input_ids\"]\n        attention_mask = inputs[\"attention_mask\"]\n        \n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        embeddings = outputs.pooler_output\n        anchor = embeddings[::2]\n        positive = embeddings[1::2]\n        \n        loss = self.contrastive_loss_fn(anchor, positive)\n\n        return (loss, outputs) if return_outputs else loss\n\n    def prediction_step(self, model, inputs, prediction_loss_only=False, ignore_keys=None):\n        if \"labels\" in inputs:\n            del inputs[\"labels\"]\n        \n        return super().prediction_step(model, inputs, prediction_loss_only, ignore_keys)\n\n# Load dataset\ndataset = load_dataset(\"imdb\", split=\"train[:10%]\")\ndataset = dataset.map(lambda x: simcse_tokenizer(x['text'], padding=True, truncation=True, max_length=512), batched=True)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=200,\n    evaluation_strategy=\"steps\",\n    save_strategy=\"steps\",\n)\n\ntrain_data = load_dataset(\"imdb\", split=\"train[:90%]\")\neval_data = load_dataset(\"imdb\", split=\"train[90%:]\")\n\ntrain_data = train_data.map(lambda x: simcse_tokenizer(x['text'], padding=True, truncation=True, max_length=512), batched=True)\neval_data = eval_data.map(lambda x: simcse_tokenizer(x['text'], padding=True, truncation=True, max_length=512), batched=True)\n\ntrainer = SimCSETrainer(\n    model=simcse_model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=eval_data,\n    contrastive_loss_fn=ContrastiveLoss(temperature=0.05),\n)\n\ntrainer.train()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_encodings = simcse_tokenizer(train_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\nval_encodings = simcse_tokenizer(val_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SimCSEMovieReviewDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = SimCSEMovieReviewDataset(train_encodings, train_labels)\nval_dataset = SimCSEMovieReviewDataset(val_encodings, val_labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, EarlyStoppingCallback, DataCollatorWithPadding\n\nmodel = DistilBertForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased-finetuned-sst-2-english\", num_labels=2\n)\n\noptimizer = AdamW(\n    model.parameters(),\n    lr=2e-5\n)\n\nlr_scheduler = get_cosine_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=100,\n    num_training_steps=len(train_texts) * 2\n)\n\nconfig = DistilBertConfig.from_pretrained(\n    \"distilbert-base-uncased-finetuned-sst-2-english\", \n    num_labels=2, \n    dropout=0.2,\n    attention_dropout=0.2\n)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"steps\",\n    save_strategy=\"steps\",\n    logging_dir=\"./logs\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=2,\n    num_train_epochs=5,\n    warmup_steps=200,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    eval_steps=500,\n    logging_steps=500,\n    learning_rate=1e-5,\n    weight_decay=0.01,\n)\n\naccuracy_metric = load_metric(\"accuracy\")\nf1_metric = load_metric(\"f1\")\nprecision_metric = load_metric(\"precision\")\nrecall_metric = load_metric(\"recall\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n    return {\n        \"accuracy\": accuracy['accuracy'],\n        \"f1\": f1['f1'],\n        \"precision\": precision['precision'],\n        \"recall\": recall['recall']\n    }\n\nearly_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n    optimizers=(optimizer, lr_scheduler),\n    callbacks=[early_stopping]\n)","metadata":{"_uuid":"89be7465-2c20-431c-bb8b-cb0f1c8d5d81","_cell_guid":"483c283e-55a4-4f1f-9f6c-0fef91c84171","trusted":true,"id":"0z6re1avQ52u","outputId":"b70dd8cc-5185-4535-bda0-0f2f247cf280"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Huấn luyện mô hình\ntrainer.train()","metadata":{"_uuid":"3e2d8fca-9acc-4932-989a-7338eb150a92","_cell_guid":"c2d52c75-e01f-44c8-978e-2b8dcf421fb0","trusted":true,"collapsed":false,"id":"udJJ0t_F2JOq","outputId":"0827bf0d-0289-41d0-eed8-8df18396cbae","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_sentiment(text):\n    inputs = simcse_tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n    \n    inputs = {key: value for key, value in inputs.items() if key != 'token_type_ids'}\n\n    for key in inputs:\n        inputs[key] = inputs[key].to(model.device)\n\n    outputs = model(**inputs)\n    prediction = torch.argmax(outputs.logits, dim=-1)\n    return \"Positive\" if prediction.item() == 1 else \"Negative\"","metadata":{"_uuid":"4635a74b-5df8-4aaf-9cb9-6c285ecb91ec","_cell_guid":"f35df774-bb5e-4323-bc68-96e4d49dcf70","trusted":true,"collapsed":false,"id":"inIQPV3VRBL1","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sentence = \"So hot today =_=  don`t like it and i hate my new timetable, having such a bad week\"\nprint(\"Original sentence: \" + sentence)\nprint(\"Predict before: \" + predict_sentiment(sentence))\nsentence = process(sentence)\nprint(\"After processing sentence: \" + sentence)\nprint(\"Predict after: \" + predict_sentiment(sentence))","metadata":{"_uuid":"a3bd9814-39fc-4e6a-9a9c-d11fcf231876","_cell_guid":"7a158f37-5fd1-4828-93f1-77e7ab84e220","trusted":true,"collapsed":false,"id":"itYIq8KWOq5f","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"correct_predictions = sum(\n    1 for i in range(len(test_texts))\n    if (predict_sentiment(process(test_texts[i])) == \"Positive\") == test_labels[i]\n)\n\naccuracy = correct_predictions / len(test_labels)\nprint(accuracy)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}